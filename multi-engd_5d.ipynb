{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-02T01:56:45.418223403Z",
     "start_time": "2024-01-02T01:50:50.843568552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGD Iteration: 10\n",
      "  with loss: 54.68884812344515 + 21.610013825782787 = 76.29886194922793\n",
      "  with relative error L2: 1.1223948956260013 and error H1: 1.1769769744652028\n",
      "  with step: 0.125\n",
      "ENGD Iteration: 20\n",
      "  with loss: 0.9876317099300519 + 0.22495603033443942 = 1.2125877402644913\n",
      "  with relative error L2: 0.1370635770226631 and error H1: 0.15297079350272477\n",
      "  with step: 0.5\n",
      "ENGD Iteration: 30\n",
      "  with loss: 0.00023352589897040465 + 4.883026347165704e-05 = 0.0002823561624420617\n",
      "  with relative error L2: 0.0016446045420251674 and error H1: 0.002677029323014301\n",
      "  with step: 0.5\n",
      "ENGD Iteration: 40\n",
      "  with loss: 6.22047021627388e-07 + 1.484096927285855e-07 = 7.704567143559735e-07\n",
      "  with relative error L2: 0.00010587898406792481 and error H1: 0.000126925306971915\n",
      "  with step: 1.0\n",
      "ENGD Iteration: 50\n",
      "  with loss: 1.2978669541001043e-10 + 6.506143196306849e-12 = 1.362928386063173e-10\n",
      "  with relative error L2: 3.812423750593745e-07 and error H1: 2.4856219170818905e-06\n",
      "  with step: 1.0\n",
      "ENGD Iteration: 60\n",
      "  with loss: 1.177013826040497e-10 + 6.178117987216515e-12 = 1.238795005912662e-10\n",
      "  with relative error L2: 3.4121234389206355e-07 and error H1: 2.4589446012284056e-06\n",
      "  with step: 0.25\n",
      "ENGD Iteration: 70\n",
      "  with loss: 1.1770174132738137e-10 + 6.177551620469277e-12 = 1.2387929294785065e-10\n",
      "  with relative error L2: 3.4108237925818605e-07 and error H1: 2.4587818914945806e-06\n",
      "  with step: 0.5\n",
      "ENGD Iteration: 80\n",
      "  with loss: 1.1770073761430612e-10 + 6.178499449295524e-12 = 1.2387923706360164e-10\n",
      "  with relative error L2: 3.411013689084628e-07 and error H1: 2.4589528537737685e-06\n",
      "  with step: 0.5\n",
      "ENGD Iteration: 90\n",
      "  with loss: 1.1770444557415448e-10 + 6.174704837271641e-12 = 1.2387915041142612e-10\n",
      "  with relative error L2: 3.4100831256383026e-07 and error H1: 2.4582014220456287e-06\n",
      "  with step: 1.0\n",
      "ENGD Iteration: 100\n",
      "  with loss: 1.1770050947417257e-10 + 6.178608169772555e-12 = 1.2387911764394513e-10\n",
      "  with relative error L2: 3.4112090641482293e-07 and error H1: 2.4589768240031424e-06\n",
      "  with step: 0.5\n",
      "ENGD Iteration: 110\n",
      "  with loss: 1.177003868633879e-10 + 6.178631098447942e-12 = 1.2387901796183586e-10\n",
      "  with relative error L2: 3.4112429564683834e-07 and error H1: 2.4590263533692478e-06\n",
      "  with step: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 122\u001B[0m\n\u001B[1;32m    119\u001B[0m interior_nat_grads \u001B[38;5;241m=\u001B[39m nat_grad(params, interior_grads)\n\u001B[1;32m    121\u001B[0m boundary_grads \u001B[38;5;241m=\u001B[39m grad(boundary_loss)(params)\n\u001B[0;32m--> 122\u001B[0m boundary_nat_grads \u001B[38;5;241m=\u001B[39m \u001B[43mnat_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mboundary_grads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m updates \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mtree_util\u001B[38;5;241m.\u001B[39mtree_map(\u001B[38;5;28;01mlambda\u001B[39;00m i, b: (i \u001B[38;5;241m+\u001B[39m b) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m, interior_nat_grads, boundary_nat_grads)\n\u001B[1;32m    126\u001B[0m params, actual_step \u001B[38;5;241m=\u001B[39m ls_update(params, updates)\n",
      "File \u001B[0;32m~/Projects/miniproject/Natural-Gradient-PINNs-ICML23/ngrad/gram.py:50\u001B[0m, in \u001B[0;36mnat_grad_factory.<locals>.natural_gradient\u001B[0;34m(params, tangent_params)\u001B[0m\n\u001B[1;32m     47\u001B[0m flat_tangent, retriev_pytree  \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mflatten_util\u001B[38;5;241m.\u001B[39mravel_pytree(tangent_params)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# solve gram dot flat_tangent.\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m flat_nat_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlstsq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgram_matrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat_tangent\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# if gramian is zero then lstsq gives back nan...\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m jnp\u001B[38;5;241m.\u001B[39misnan(flat_nat_grad[\u001B[38;5;241m0\u001B[39m]):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/linalg.py:680\u001B[0m, in \u001B[0;36mlstsq\u001B[0;34m(a, b, rcond, numpy_resid)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m numpy_resid:\n\u001B[1;32m    679\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _lstsq(a, b, rcond, numpy_resid\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_jit_lstsq\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrcond\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ENGD Optimization.\n",
    "Five dimensional Poisson equation example. Solution given by\n",
    "\n",
    "u(x) = sum_{i=1}^5 sin(pi * x_i)\n",
    "\n",
    "\"\"\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, vmap, jit\n",
    "\n",
    "from ngrad.domains import Hyperrectangle, HypercubeBoundary\n",
    "from ngrad.models import mlp, init_params\n",
    "from ngrad.integrators import EvolutionaryIntegrator\n",
    "from ngrad.utility import laplace, grid_line_search_factory\n",
    "from ngrad.inner import model_laplace, model_identity\n",
    "from ngrad.gram import gram_factory, nat_grad_factory\n",
    "\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# random seed\n",
    "seed = 0\n",
    "\n",
    "# domains\n",
    "dim = 3\n",
    "interior = Hyperrectangle([(0., 1.) for _ in range(0, dim)])\n",
    "boundary = HypercubeBoundary(dim)\n",
    "\n",
    "# integrators\n",
    "interior_integrator = EvolutionaryIntegrator(interior, key=random.PRNGKey(0), N=4000)\n",
    "boundary_integrator = EvolutionaryIntegrator(boundary, key= random.PRNGKey(1), N=500)\n",
    "eval_integrator = EvolutionaryIntegrator(interior, key=random.PRNGKey(0), N= 10 * 4000)\n",
    "\n",
    "# model\n",
    "activation = lambda x : jnp.tanh(x)\n",
    "layer_sizes = [dim, 64, 1]\n",
    "params = init_params(layer_sizes, random.PRNGKey(seed))\n",
    "model = mlp(activation)\n",
    "v_model = vmap(model, (None, 0))\n",
    "\n",
    "# solution\n",
    "@jit\n",
    "def u_star(x):\n",
    "    return (jnp.sum(jnp.sin(jnp.pi * x)))\n",
    "\n",
    "v_u_star = vmap(u_star, (0))\n",
    "v_grad_u_star = vmap(\n",
    "    lambda x: jnp.dot(grad(u_star)(x), grad(u_star)(x))**0.5, (0)\n",
    "    )\n",
    "\n",
    "# rhs\n",
    "@jit\n",
    "def f(x):\n",
    "    return jnp.pi**2 * u_star(x)\n",
    "\n",
    "# gramians\n",
    "gram_bdry = gram_factory(\n",
    "    model = model,\n",
    "    trafo = model_identity,\n",
    "    integrator = boundary_integrator\n",
    ")\n",
    "\n",
    "gram_laplace = gram_factory(\n",
    "    model = model,\n",
    "    trafo = model_laplace,\n",
    "    integrator = interior_integrator\n",
    ")\n",
    "\n",
    "@jit\n",
    "def gram(params):\n",
    "    return (gram_bdry(params) + gram_laplace(params))\n",
    "\n",
    "# natural gradient\n",
    "nat_grad = nat_grad_factory(gram)\n",
    "\n",
    "# compute residual\n",
    "laplace_model = lambda params: laplace(lambda x: model(params, x))\n",
    "residual = lambda params, x: (laplace_model(params)(x) + f(x))**2.\n",
    "v_residual =  jit(vmap(residual, (None, 0)))\n",
    "\n",
    "# loss\n",
    "@jit\n",
    "def interior_loss(params):\n",
    "    return 0.5 * interior_integrator(lambda x: v_residual(params, x))\n",
    "\n",
    "@jit\n",
    "def boundary_loss(params):\n",
    "    return (\n",
    "        0.5 * boundary_integrator(lambda x: (v_model(params, x) - v_u_star(x))**2.)\n",
    "    )\n",
    "\n",
    "@jit\n",
    "def loss(params):\n",
    "    return interior_loss(params) + boundary_loss(params)\n",
    "\n",
    "# set up grid line search\n",
    "grid = jnp.linspace(0, 30, 31)\n",
    "steps = 0.5**grid\n",
    "ls_update = grid_line_search_factory(loss, steps)\n",
    "\n",
    "# errors\n",
    "error = lambda x: model(params, x) - u_star(x)\n",
    "v_error = vmap(error, (0))\n",
    "v_error_abs_grad = vmap(\n",
    "        lambda x: jnp.dot(grad(error)(x), grad(error)(x))**0.5\n",
    "        )\n",
    "\n",
    "def l2_norm(f, integrator):\n",
    "    return integrator(lambda x: (f(x))**2)**0.5\n",
    "\n",
    "norm_sol_l2 = l2_norm(v_u_star, eval_integrator)\n",
    "norm_sol_h1 = norm_sol_l2 + l2_norm(v_grad_u_star, eval_integrator)    \n",
    "\n",
    "\n",
    "# training loop\n",
    "for iteration in range(201):\n",
    "    interior_grads = grad(interior_loss)(params)\n",
    "    interior_nat_grads = nat_grad(params, interior_grads)\n",
    "    \n",
    "    boundary_grads = grad(boundary_loss)(params)\n",
    "    boundary_nat_grads = nat_grad(params, boundary_grads)\n",
    "    \n",
    "    updates = jax.tree_util.tree_map(lambda i, b: (i + b) / 2, interior_nat_grads, boundary_nat_grads)\n",
    "    \n",
    "    params, actual_step = ls_update(params, updates)\n",
    "\n",
    "    if iteration % 10 == 0 and iteration > 0:\n",
    "        l2_error = l2_norm(v_error, eval_integrator)\n",
    "        h1_error = l2_error + l2_norm(v_error_abs_grad, eval_integrator)\n",
    "\n",
    "        print(\n",
    "            f'ENGD Iteration: {iteration}'\n",
    "            f'\\n  with loss: {interior_loss(params)} + {boundary_loss(params)} = {loss(params)}'\n",
    "            f'\\n  with relative errors L2: {l2_error/norm_sol_l2} and H1: {h1_error/norm_sol_h1}'\n",
    "            f'\\n  with step: {actual_step}'\n",
    "        )\n",
    "\n",
    "    # draw new points -- this can slow down the optimization\n",
    "    if iteration % 1 == 0:\n",
    "        interior_integrator.new_rand_points()\n",
    "        boundary_integrator.new_rand_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4bb42b58ca2d7548"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
