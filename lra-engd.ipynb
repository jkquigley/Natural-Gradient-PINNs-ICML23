{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGD Iteration: 0\n",
      "  with loss: 104.35925919813225 + 0.07558416209379887 = 104.43484336022605\n",
      "  with error L2: 0.40265589073409175 and error H1: 2.624555747403158\n",
      "  with acual step: 0.125\n",
      "ENGD Iteration: 10\n",
      "  with loss: 34.201900536079016 + 3.223910526887441 = 37.42581106296646\n",
      "  with error L2: 0.8706637395832425 and error H1: 3.112001644713624\n",
      "  with acual step: 0.00390625\n",
      "ENGD Iteration: 20\n",
      "  with loss: 16.050920153328434 + 0.3867210963871149 = 16.43764124971555\n",
      "  with error L2: 0.2173899869321513 and error H1: 1.5117851780828009\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 30\n",
      "  with loss: 14.490932268531752 + 0.28789844402766135 = 14.778830712559413\n",
      "  with error L2: 0.16525799999294777 and error H1: 1.3557505354200734\n",
      "  with acual step: 0.0019531249999999998\n",
      "ENGD Iteration: 40\n",
      "  with loss: 12.980381710768512 + 0.22569802377646683 = 13.206079734544979\n",
      "  with error L2: 0.14545893889233724 and error H1: 1.2117288831908835\n",
      "  with acual step: 0.00048828125\n",
      "ENGD Iteration: 50\n",
      "  with loss: 11.776084497686412 + 0.16287333126717357 = 11.938957828953585\n",
      "  with error L2: 0.12503844110404255 and error H1: 1.047062465529907\n",
      "  with acual step: 0.0019531249999999998\n",
      "ENGD Iteration: 60\n",
      "  with loss: 10.429276943121177 + 0.5425059892743291 = 10.971782932395506\n",
      "  with error L2: 0.34076198946750613 and error H1: 1.1279286060435112\n",
      "  with acual step: 0.03125\n",
      "ENGD Iteration: 70\n",
      "  with loss: 9.88886715257113 + 0.08395763607994736 = 9.972824788651078\n",
      "  with error L2: 0.08158375239908301 and error H1: 0.775883267386979\n",
      "  with acual step: 0.00048828125\n",
      "ENGD Iteration: 80\n",
      "  with loss: 9.4520919502732 + 0.07850369198489623 = 9.530595642258096\n",
      "  with error L2: 0.09078601814355658 and error H1: 0.7297566577903278\n",
      "  with acual step: 0.0078125\n",
      "ENGD Iteration: 90\n",
      "  with loss: 9.059070079187496 + 0.0763034289561175 = 9.135373508143614\n",
      "  with error L2: 0.08982712434107691 and error H1: 0.7018598682876441\n",
      "  with acual step: 0.0019531249999999998\n",
      "ENGD Iteration: 100\n",
      "  with loss: 8.249336329467152 + 0.1654230586935625 = 8.414759388160714\n",
      "  with error L2: 0.18010729106650863 and error H1: 0.7632477338093999\n",
      "  with acual step: 0.06249999999999999\n",
      "ENGD Iteration: 110\n",
      "  with loss: 7.806564312209925 + 0.07281431005599373 = 7.879378622265919\n",
      "  with error L2: 0.09487625003060708 and error H1: 0.6638139689010457\n",
      "  with acual step: 0.0078125\n",
      "ENGD Iteration: 120\n",
      "  with loss: 7.542909061017215 + 0.12025925517396184 = 7.663168316191177\n",
      "  with error L2: 0.1457835866135835 and error H1: 0.7165916418049795\n",
      "  with acual step: 0.015625\n",
      "ENGD Iteration: 130\n",
      "  with loss: 7.376465473128433 + 0.05040060024409753 = 7.426866073372531\n",
      "  with error L2: 0.05483659645548412 and error H1: 0.6171888881536624\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 140\n",
      "  with loss: 7.175959114876246 + 0.05120070689253174 = 7.227159821768778\n",
      "  with error L2: 0.05608407152146479 and error H1: 0.6184636753453652\n",
      "  with acual step: 0.00048828125\n",
      "ENGD Iteration: 150\n",
      "  with loss: 6.94823078133357 + 0.05200034062949917 = 7.000231121963069\n",
      "  with error L2: 0.056395179897429544 and error H1: 0.6189259136938065\n",
      "  with acual step: 0.00048828125\n",
      "ENGD Iteration: 160\n",
      "  with loss: 6.778407406511186 + 0.05281215827233001 = 6.831219564783516\n",
      "  with error L2: 0.05732343275675452 and error H1: 0.6200814895733078\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 170\n",
      "  with loss: 6.600633491210189 + 0.05313085026971388 = 6.653764341479903\n",
      "  with error L2: 0.05666484319613432 and error H1: 0.620032873027102\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 180\n",
      "  with loss: 6.415759879954129 + 0.05371882279139839 = 6.469478702745527\n",
      "  with error L2: 0.0563453835385131 and error H1: 0.6207508909141325\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 190\n",
      "  with loss: 6.225128432805583 + 0.0548965950541521 = 6.280025027859735\n",
      "  with error L2: 0.05723904682155482 and error H1: 0.6232105344616145\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 200\n",
      "  with loss: 6.028453362204261 + 0.05579897088932257 = 6.084252333093584\n",
      "  with error L2: 0.05724878030288639 and error H1: 0.625206692028785\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 210\n",
      "  with loss: 5.826331448873086 + 0.05660841516089708 = 5.882939864033983\n",
      "  with error L2: 0.05679101510429812 and error H1: 0.6271860403641827\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 220\n",
      "  with loss: 5.620710020573307 + 0.05796980308050338 = 5.678679823653811\n",
      "  with error L2: 0.05737313299674738 and error H1: 0.6308095165911014\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 230\n",
      "  with loss: 5.412124123224508 + 0.05939216693321475 = 5.471516290157723\n",
      "  with error L2: 0.05788104693102224 and error H1: 0.6349190469409709\n",
      "  with acual step: 0.0009765625\n",
      "ENGD Iteration: 240\n",
      "  with loss: 5.201016403093369 + 0.06057701336636059 = 5.26159341645973\n",
      "  with error L2: 0.05767449901703918 and error H1: 0.6388091030067641\n",
      "  with acual step: 0.0009765625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 118\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m iteration \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(iterations \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    117\u001B[0m     interior_grads \u001B[38;5;241m=\u001B[39m grad(interior_loss)(params)\n\u001B[0;32m--> 118\u001B[0m     interior_nat_grads \u001B[38;5;241m=\u001B[39m \u001B[43mnat_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterior_grads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m     boundary_grads \u001B[38;5;241m=\u001B[39m grad(boundary_loss)(params)\n\u001B[1;32m    121\u001B[0m     boundary_nat_grads \u001B[38;5;241m=\u001B[39m nat_grad(params, boundary_grads)\n",
      "File \u001B[0;32m~/Projects/miniproject/Natural-Gradient-PINNs-ICML23/ngrad/gram.py:50\u001B[0m, in \u001B[0;36mnat_grad_factory.<locals>.natural_gradient\u001B[0;34m(params, tangent_params)\u001B[0m\n\u001B[1;32m     47\u001B[0m flat_tangent, retriev_pytree  \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mflatten_util\u001B[38;5;241m.\u001B[39mravel_pytree(tangent_params)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# solve gram dot flat_tangent.\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m flat_nat_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlstsq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgram_matrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat_tangent\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# if gramian is zero then lstsq gives back nan...\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m jnp\u001B[38;5;241m.\u001B[39misnan(flat_nat_grad[\u001B[38;5;241m0\u001B[39m]):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/linalg.py:680\u001B[0m, in \u001B[0;36mlstsq\u001B[0;34m(a, b, rcond, numpy_resid)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m numpy_resid:\n\u001B[1;32m    679\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _lstsq(a, b, rcond, numpy_resid\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_jit_lstsq\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrcond\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ENGD Optimization.\n",
    "Two dimensional Poisson equation example. Solution given by\n",
    "\n",
    "u(x,y) = sin(pi*x) * sin(py*y).\n",
    "\n",
    "\"\"\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, vmap, jit\n",
    "\n",
    "from ngrad.models import init_params, mlp\n",
    "from ngrad.domains import Square, SquareBoundary\n",
    "from ngrad.integrators import DeterministicIntegrator\n",
    "from ngrad.utility import laplace, grid_line_search_factory\n",
    "from ngrad.inner import model_laplace, model_identity\n",
    "from ngrad.gram import gram_factory, nat_grad_factory\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "tau = 1.\n",
    "\n",
    "# random seed\n",
    "seed = 0\n",
    "\n",
    "# domains\n",
    "interior = Square(1.)\n",
    "boundary = SquareBoundary(1.)\n",
    "\n",
    "# integrators\n",
    "interior_integrator = DeterministicIntegrator(interior, 30)\n",
    "boundary_integrator = DeterministicIntegrator(boundary, 30)\n",
    "eval_integrator = DeterministicIntegrator(interior, 200)\n",
    "\n",
    "# model\n",
    "activation = lambda x : jnp.tanh(x)\n",
    "layer_sizes = [2, 32, 1]\n",
    "params = init_params(layer_sizes, random.PRNGKey(seed))\n",
    "model = mlp(activation)\n",
    "# v_model = vmap(model, (None, 0))\n",
    "v_model = vmap(lambda params, x: model(params, x), (None, 0))\n",
    "\n",
    "# solution\n",
    "@jit\n",
    "def u_star(x):\n",
    "    return jnp.prod(jnp.sin(jnp.pi * x))\n",
    "\n",
    "# rhs\n",
    "@jit\n",
    "def f(x):\n",
    "    return 2. * jnp.pi**2 * u_star(x)\n",
    "\n",
    "# gramians\n",
    "gram_bdry = gram_factory(\n",
    "    model = model,\n",
    "    trafo = model_identity,\n",
    "    integrator = boundary_integrator\n",
    ")\n",
    "\n",
    "gram_laplace = gram_factory(\n",
    "    model = model,\n",
    "    trafo = model_laplace,\n",
    "    integrator = interior_integrator\n",
    ")\n",
    "\n",
    "@jit\n",
    "def gram(params):\n",
    "    return gram_laplace(params) + gram_bdry(params)\n",
    "\n",
    "# natural gradient\n",
    "nat_grad = nat_grad_factory(gram)\n",
    "\n",
    "# compute residual\n",
    "laplace_model = lambda params: laplace(lambda x: model(params, x))\n",
    "residual = lambda params, x: (laplace_model(params)(x) + f(x))**2.\n",
    "v_residual =  jit(vmap(residual, (None, 0)))\n",
    "\n",
    "# loss\n",
    "@jit\n",
    "def interior_loss(params):\n",
    "    return interior_integrator(lambda x: v_residual(params, x))\n",
    "\n",
    "@jit\n",
    "def boundary_loss(params):\n",
    "    return tau * boundary_integrator(lambda x: v_model(params, x)**2)\n",
    "\n",
    "@jit\n",
    "def loss(params):\n",
    "    return interior_loss(params) + boundary_loss(params)\n",
    "\n",
    "# set up grid line search\n",
    "grid = jnp.linspace(0, 30, 31)\n",
    "steps = 0.5**grid\n",
    "ls_update = grid_line_search_factory(loss, steps)\n",
    "\n",
    "# errors\n",
    "error = lambda x: model(params, x) - u_star(x)\n",
    "v_error = vmap(error, (0))\n",
    "v_error_abs_grad = vmap(\n",
    "        lambda x: jnp.dot(grad(error)(x), grad(error)(x))**0.5\n",
    "        )\n",
    "\n",
    "def l2_norm(f, integrator):\n",
    "    return integrator(lambda x: (f(x))**2)**0.5    \n",
    "\n",
    "\n",
    "iterations = 1000\n",
    "save_freq = 10\n",
    "\n",
    "import numpy as np\n",
    "data = np.empty((iterations // save_freq + 1, 5))\n",
    "\n",
    "# natural gradient descent with line search\n",
    "alpha = 0.1\n",
    "wb = 1.\n",
    "for iteration in range(iterations + 1):\n",
    "    interior_grads = grad(interior_loss)(params)\n",
    "    interior_nat_grads = nat_grad(params, interior_grads)\n",
    "    \n",
    "    boundary_grads = grad(boundary_loss)(params)\n",
    "    boundary_nat_grads = nat_grad(params, boundary_grads)\n",
    "    \n",
    "    updates = jax.tree_util.tree_map(\n",
    "        lambda i, b: i + wb * b,\n",
    "        interior_grads,\n",
    "        boundary_grads,\n",
    "    )\n",
    "    params, actual_step = ls_update(params, updates)\n",
    "    \n",
    "    if iteration % save_freq == 0:\n",
    "        # errors\n",
    "        l2_error = l2_norm(v_error, eval_integrator)\n",
    "        h1_error = l2_error + l2_norm(v_error_abs_grad, eval_integrator)\n",
    "        \n",
    "        data[iteration // save_freq, :] = [\n",
    "            iteration,\n",
    "            interior_loss(params),\n",
    "            boundary_loss(params),\n",
    "            l2_error,\n",
    "            h1_error,\n",
    "        ]\n",
    "    \n",
    "        print(\n",
    "            f'ENGD Iteration: {iteration}'\n",
    "            f'\\n  with loss: {interior_loss(params)} + {boundary_loss(params)} = {loss(params)}'\n",
    "            f'\\n  with error L2: {l2_error} and error H1: {h1_error}'\n",
    "            f'\\n  with acual step: {actual_step}'\n",
    "        )\n",
    "        \n",
    "    interior_grads_raveled, _ = jax.flatten_util.ravel_pytree(interior_grads)\n",
    "    boundary_grads_raveled, _ = jax.flatten_util.ravel_pytree(boundary_grads)\n",
    "\n",
    "    # update loss weights\n",
    "    wb_hat = len(boundary_grads_raveled) * jnp.max(jnp.abs(interior_grads_raveled)) / (wb * jnp.sum(jnp.abs(boundary_grads_raveled)))\n",
    "    wb = (1 - alpha) * wb + alpha * wb_hat\n",
    "        \n",
    "# jnp.save(\"data/engd.npy\", data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:47:43.513302285Z",
     "start_time": "2024-01-02T06:46:37.431699108Z"
    }
   },
   "id": "8de4bfe8b7d5b442"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T06:43:32.764308656Z"
    }
   },
   "id": "2892e2c57612f349"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "15153c9fe2cc0fb4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
